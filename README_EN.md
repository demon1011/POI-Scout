# POI-Scout

[简体中文](./README.md) | English

> "I want to find a fun place to take my kids this weekend" — With just one sentence, POI-Scout searches the web and finds the perfect destination for you.

An intelligent travel destination search Agent that supports fuzzy requirement understanding, self-optimizing search, and interactive filtering.

<table>
<tr>
<td width="50%">

**Main Interface**: Create new tasks or load existing ones
![Main Interface](docs/images/main_interface.png)

</td>
<td width="50%">

**Selection Interface**: Quickly converge to ideal POI through Q&A
![Selection Interface](docs/images/select_interface.png)

</td>
</tr>
</table>

## Core Features

| Feature | Description |
|---------|-------------|
| **Multi-step Search Planning** | Automatically decomposes vague requirements into multiple search themes to maximize coverage of potential needs |
| **Online Optimization (On-policy)** | Based on [TextGrad](https://arxiv.org/abs/2406.07496)'s Reflection mechanism, iteratively improves search plans in real-time |
| **Offline Experience Reuse (Off-policy)** | Inspired by [GEPA](https://arxiv.org/abs/2507.19457), extracts skills from historical tasks for new tasks |
| **Bayesian Decision Tree Filtering** | Based on [Bayesian experimental methods](https://arxiv.org/abs/2508.21184), quickly converges to ideal POI through multi-round Q&A |

> **About On/Off-policy terminology**: Here, "policy" refers to the search plan generated by the Agent for a specific request. On-policy uses feedback from the current plan to improve the same plan; Off-policy uses experience from other tasks to assist the current task.

## Quick Start

```bash
# Clone and install
git clone https://github.com/demon1011/POI-Scout.git
cd POI-Scout
pip install -r requirements.txt
playwright install chromium

# Configure API keys
cp config.example.py config.py
# Edit config.py and fill in your SiliconFlow and Bocha Search API keys

# Run
python main.py                          # Basic search
python main.py --online-opt             # Enable online optimization
python main.py --use-skill              # Use historical experience
python main.py --online-opt --use-skill --create-skill  # Full mode
```

**API Dependencies**: [SiliconFlow](https://siliconflow.cn) (LLM + Embedding), [Bocha Search](https://bochaai.com) (Web Search)

## Technical Approach

### Online Optimization (On-policy)

Search tools are black boxes to the model — the model cannot predict what kind of search requests will yield more effective POIs. To address this, we designed an iterative optimization mechanism based on Reflection:

1. **Execute & Evaluate**: Agent executes the initial search plan, recording POI recall quantity and quality for each step
2. **Problem Identification**: Through Reflection, the model analyzes which steps are inefficient and identifies shortcomings in search requests
3. **Plan Improvement**: Generates specific modification suggestions for inefficient steps, adjusting search keywords and expanding search angles
4. **Iterative Execution**: Re-executes search with the improved plan, compares results, and continues optimization

To prevent optimization from getting stuck in local optima, we introduced a **regularization strategy**: when a step shows poor optimization results multiple times consecutively, a re-sampling mechanism is triggered to explore from new directions.

![Optimization Process](docs/images/poi_optimization.png)
*Optimization curve example (query: "kid-friendly places in Jinhua")*

### Offline Experience Extraction (Off-policy)

Online optimization accumulates a wealth of tacit knowledge about "what improvements work." How can we preserve this for future tasks?

We draw inspiration from **GAE (Generalized Advantage Estimation)** in reinforcement learning: by comparing the plans and effect differences before and after optimization, we calculate the "advantage" of each improvement. Unlike numerical GAE, we use **natural language** to let the model summarize these advantages, forming readable and reusable skills:

- Compare search plan differences before and after optimization
- Analyze changes in POI recall quantity and quality
- Extract general experience rules (e.g., "when searching for family attractions, consider both indoor and outdoor venues")

Experience deduplication and diversity filtering through Embedding to avoid redundancy.

**Experimental Results** (n=15): POI recall improved by **17.3%** (30.1→35.3) after using skills, p=0.035, Cohen's d=0.815.

![Experience Effect Comparison](docs/images/boxplot_comparison.png)

## Project Structure

```
POI-Scout/
├── main.py              # Entry point
├── config.py            # API key configuration
├── src/
│   ├── agent/           # ReAct Agent implementation
│   ├── search/          # Search and optimization logic
│   ├── selector/        # Decision tree filtering
│   └── tools/           # Crawler and search tools
│       ├── crawler.py       # Playwright web crawler
│       ├── crawl_logger.py  # Crawler logging module
│       └── tools.py         # Search tool wrappers
└── data/
    ├── skills/          # Offline experience storage
    └── crawl_logs/      # Crawler logs (JSON Lines format)
```

### Crawler Logging

The crawler module records detailed information for each crawl in real-time, enabling analysis and optimization:

- **Log files**: `data/crawl_logs/crawl_log_YYYYMMDD_HHMMSS.jsonl`
- **Summary**: `data/crawl_logs/crawl_log_YYYYMMDD_HHMMSS_summary.json`

Recorded fields include: URL, success/failure status, error type (timeout/network/blocked, etc.), response time, content length, domain statistics, and more.

## License

MIT License

## Contact

Email: 184380405@qq.com
